# Онбординг: обзор проекта и контекст рефакторинга

Краткая шпаргалка для LLM-агента (или человека), который подключается к репозиторию на **другой машине** (например, VM с Vertex AI): что это за проект, куда смотреть, что копировать и как устроен доступ к LLM.

---

## 1. Что это за проект

**English Words** — пайплайн, который превращает **книги** (epub/текст) в **карточки Anki** для изучения английского (уровень B1–B2).

- **Вход:** текст из книг (токены, предложения, частотная статистика).
- **Выход:** JSON/CSV «умных карточек»: лемма, определение, перевод, до 3 примеров (из книги и сгенерированных).
- **Ядро:** LLM используется для: снятия неоднозначности смысла (WSD), агрегации synset’ов, проверки примеров, генерации карточек (определения, переводы, выбор примеров).

Упрощённая схема:

```
Текст книги → Токены/статистика → WSD (смысл на слово) → Агрегация synset → Генерация карточек (LLM) → Экспорт в Anki
```

Подробнее: `docs/WORD_FAMILY_PIPELINE.md`, `docs/QUALITY_FILTERING_PLAN.md`, `README.md`.

---

## 2. Куда смотреть (структура)

| Область | Путь | Описание |
|--------|------|----------|
| **Точка входа пайплайна** | `scripts/run_synset_card_generation.py` | Основной скрипт: агрегация + генерация карточек. |
| **Слой LLM** | `src/eng_words/llm/` | `base.py` (интерфейс), `providers/` (OpenAI, Anthropic, **Gemini**), `response_cache.py`, `smart_card_generator.py`, `retry.py`. |
| **Агрегация** | `src/eng_words/aggregation/` | LLM группирует synset’ы WordNet в одну карточку на значение. |
| **WSD** | `src/eng_words/wsd/` | Снятие неоднозначности смысла (LLM + WordNet). |
| **Валидация** | `src/eng_words/validation/` | Проверка примеров из книги для synset’а (LLM). |
| **Конфиг / цены** | `src/eng_words/constants/llm_pricing.py`, `defaults.py`, `files.py` | Модели, пути, оценка стоимости. |
| **Документация** | `docs/` | `WORD_FAMILY_PIPELINE.md`, `QUALITY_FILTERING_PLAN.md`, `REFACTOR_AND_BACKUP_PLAN.md`, `GENERATION_INSTRUCTIONS.md`, `LLM_API_KEYS_SETUP.md`. |
| **Архив** | `src/eng_words/_archive/`, `scripts/_archive/`, `tests/_archive/`, `docs/archive/` | Старый код/скрипты/доки; при рефакторинге можно не трогать. |

**Текущий «результат» пайплайна:** сгенерированные карточки лежат в выходных каталогах данных (например, `synset_smart_cards_final.json`). В репозиторий **не коммитятся** `data/`, `logs/`, `reports/` (см. `.gitignore`). То есть актуальный результат есть только на той машине, где пайплайн запускали (или в бэкапах). Для рефакторинга достаточно кода и тестов.

---

## 3. Что копировать при переносе на другую машину

| Задача | Что копировать |
|--------|----------------|
| **Только рефакторинг (без запусков)** | Ничего. Клонировать репо + `uv sync`; тесты на моках. |
| **Запуск пайплайна на VM (Vertex)** | Входные данные (§3.1) + окружение Vertex (§5). Кэш не нужен. |
| **Запуск пайплайна на VM без API (из кэша)** | Входные данные + **LLM-кэш** (§3.2). Те же промпты → попадания в кэш → без вызовов API. |

**Для рефакторинга копировать данные не нужно.** Репозиторий (код + тесты) самодостаточен; тесты мокают вызовы LLM.

### 3.1 Полный прогон пайплайна на небольшой выборке (что копировать)

Основной скрипт — `scripts/run_synset_card_generation.py`. Ему нужны:

| Что | Путь | Описание |
|-----|------|----------|
| **Агрегированные карточки** | `data/synset_aggregation_full/aggregated_cards.parquet` | Готовые группы synset’ов. Для малой выборки можно взять первые N строк. |
| **Токены книги** | `data/processed/american_tragedy_tokens.parquet` | Токены книги для восстановления предложений-примеров. В скрипте заданы `BOOK_NAME = "american_tragedy"` и `TOKENS_PATH` — пути должны совпадать. |

На VM создать эти каталоги и скопировать файлы (или урезанную выборку). Запуск с лимитом, например: `uv run python scripts/run_synset_card_generation.py 20` (первый аргумент — макс. число карточек).

### 3.2 Два варианта запуска: с Vertex и без API (повтор из кэша)

**Вариант A — с Vertex AI (реальные вызовы LLM)**  
- Копировать только **входные данные** выше (например, `aggregated_cards.parquet` и `american_tragedy_tokens.parquet` в `data/synset_aggregation_full/` и `data/processed/`).  
- Настроить окружение Vertex (§5).  
- Запустить скрипт (например, с небольшим `limit`). Кэш не нужен; каждый промпт пойдёт в Vertex.

**Вариант B — без API (повтор из кэша)**  
- Скопировать **те же входные данные** и **LLM-кэш** с машины, где уже гоняли ту же выборку: каталог **`data/synset_cards/llm_cache/`** (используется в `run_synset_card_generation.py`: `CACHE_DIR = OUTPUT_DIR / "llm_cache"` → `data/synset_cards/llm_cache`).  
- На VM запустить скрипт с тем же лимитом (например, 20). Промпты совпадут → ключ кэша (hash модели, промпта, температуры) совпадёт → попадания в кэш, вызовов LLM не будет.

Итого: чтобы гонять полный пайплайн на малой выборке без вызовов LLM, нужно скопировать (1) входные parquet для этой выборки и (2) содержимое `data/synset_cards/llm_cache/` с прогона на той же выборке. Можно скопировать и весь кэш с большего прогона — лишние файлы не мешают.

---

## 4. Цели рефакторинга

- **Стабилизировать и упростить** кодовую базу без расширения функциональности.
- **Сохранить поведение:** те же входы → те же выходы; тесты (на моках) должны оставаться зелёными.
- **Единая граница LLM:** все реальные вызовы идут через `LLMProvider` (`complete` / `complete_json`). Рефакторинг не меняет этот контракт; только внутреннюю структуру, имена, раскладку файлов и документацию.
- **Справка:** `docs/REFACTOR_AND_BACKUP_PLAN.md` описывает последнюю уборку (архивы, количество скриптов и документов). По нему видно, что «активно», а что в архиве.

Код в архиве рефакторить не обязательно, если нет конкретной задачи.

---

## 5. Доступ к LLM: эта машина и другая (Vertex AI)

**Текущая машина (например, ноутбук):**  
Используется **Google AI Studio** (Gemini API) с **API-ключом** в `GOOGLE_API_KEY`.  
Код: `src/eng_words/llm/providers/gemini.py` → `genai.Client(api_key=...)`.

**Другая машина (например, VM):**  
Нужен **Vertex AI** (те же модели Gemini, но проект GCP, без API-ключа; авторизация через Application Default Credentials).

**Где это реализовано:**

- **Файл:** `src/eng_words/llm/providers/gemini.py`
- **Конструктор:** `GeminiProvider(..., vertexai=True, project="...", location="...")` или через переменные окружения.
- **Переменные для Vertex (без API-ключа):**
  - `GOOGLE_GENAI_USE_VERTEXAI=true`
  - `GOOGLE_CLOUD_PROJECT=<your-gcp-project-id>`
  - `GOOGLE_CLOUD_LOCATION=us-central1` (или свой регион)
- **Фабрика:** `get_provider("gemini", vertexai=True, project="...", location="...")` в `src/eng_words/llm/base.py`.

Итого: **тот же код**; на VM выставить переменные Vertex (или передать `vertexai=True` + project/location), и тот же пайплайн будет вызывать Gemini через Vertex AI. Отдельного «второго» Google-бэкенда нет — один провайдер с двумя способами создания клиента (API key vs Vertex).

---

## 6. Чек-лист для другой машины

1. Клонировать репозиторий.
2. Установить зависимости: `uv sync` (или `pip install -e ".[llm]"` и т.д. по README).
3. Для Vertex AI: выставить `GOOGLE_GENAI_USE_VERTEXAI=true`, `GOOGLE_CLOUD_PROJECT`, `GOOGLE_CLOUD_LOCATION`; убедиться, что ADC работает (сервисный аккаунт или `gcloud auth application-default login`).
4. Запустить тесты: `pytest` (используются моки, вызовов LLM нет).
5. По желанию один раз прогнать пайплайн: например, `scripts/run_synset_card_generation.py` с маленьким входом, чтобы убедиться, что Vertex отвечает.

После этого можно спокойно рефакторить: те же интерфейсы, зелёные тесты, LLM за одной абстракцией и настраивается под окружение.
