#!/usr/bin/env python3
"""
–†—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫ –≠—Ç–∞–ø–∞ 2.5.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ 115 —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º:
1. –†–æ–≤–Ω–æ 3 –ø—Ä–∏–º–µ—Ä–∞
2. –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –±–µ–∑ —Å–ø–æ–π–ª–µ—Ä–æ–≤ (—É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –î–û –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
3. –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã (<=50 —Å–ª–æ–≤)
4. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–µ (<=15 —Å–ª–æ–≤)
5. –ï—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥
6. –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç synset_group (—É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –î–û –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
7. –ü—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–Ω–∏–≥–∏ + —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞

–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –±–∞—Ç—á–∞–º–∏ –ø–æ 10 –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞.
"""

import json
import logging
import sys
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from eng_words.llm.base import get_provider
from eng_words.llm.response_cache import ResponseCache
from eng_words.text_processing import create_sentences_dataframe, reconstruct_sentences_from_tokens

load_dotenv()

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger(__name__)

# Paths
BOOK_NAME = "american_tragedy"
AGGREGATED_CARDS_PATH = Path("data/synset_aggregation_full/aggregated_cards.parquet")
TOKENS_PATH = Path(f"data/processed/{BOOK_NAME}_tokens.parquet")
TEST_RESULTS_PATH = Path("data/stage2_5_test/test_results.json")
FULL_CARDS_PATH = Path("data/stage2_5_test/test_results_with_full_cards.json")
OUTPUT_DIR = Path("data/stage2_5_test")
CACHE_DIR = OUTPUT_DIR / "llm_cache"

BATCH_SIZE = 10  # –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ø–æ 10 –∫–∞—Ä—Ç–æ—á–µ–∫ –∑–∞ —Ä–∞–∑


def count_words(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤."""
    return len(text.split()) if text else 0


def check_card_quality(card_data: dict, card_index: int) -> dict:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏.

    Returns:
        dict with quality checks:
        - has_3_examples: bool
        - all_examples_appropriate_length: bool
        - definition_short: bool
        - has_translation: bool
        - issues: list[str] - —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º
    """
    issues = []
    checks = {
        "card_index": card_index,
        "lemma": card_data.get("lemma", ""),
        "pos": card_data.get("pos", ""),
    }

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –†–æ–≤–Ω–æ 3 –ø—Ä–∏–º–µ—Ä–∞
    total_count = card_data.get("total_examples_count", 0)
    if total_count == 3:
        checks["has_3_examples"] = True
    else:
        checks["has_3_examples"] = False
        issues.append(f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total_count} (–æ–∂–∏–¥–∞–µ—Ç—Å—è 3)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã (<=50 —Å–ª–æ–≤)
    max_length = card_data.get("selected_examples_max_length", 0)
    if max_length <= 50:
        checks["all_examples_appropriate_length"] = True
    else:
        checks["all_examples_appropriate_length"] = False
        issues.append(f"–ï—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–ª–∏–Ω–Ω–µ–µ 50 —Å–ª–æ–≤: {max_length}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    gen_max_length = card_data.get("generated_examples_max_length", 0)
    if gen_max_length > 50:
        checks["all_examples_appropriate_length"] = False
        issues.append(f"–ï—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª–∏–Ω–Ω–µ–µ 50 —Å–ª–æ–≤: {gen_max_length}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–µ (<=15 —Å–ª–æ–≤)
    def_length = card_data.get("definition_length", 0)
    if def_length <= 15:
        checks["definition_short"] = True
    else:
        checks["definition_short"] = False
        issues.append(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ: {def_length} —Å–ª–æ–≤ (–ª–∏–º–∏—Ç 15)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –ï—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥
    has_translation = card_data.get("has_translation", False)
    checks["has_translation"] = has_translation
    if not has_translation:
        issues.append("–ù–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 5: –ë–∞–ª–∞–Ω—Å –ø—Ä–∏–º–µ—Ä–æ–≤
    selected_count = card_data.get("selected_examples_count", 0)
    generated_count = card_data.get("generated_examples_count", 0)
    checks["selected_count"] = selected_count
    checks["generated_count"] = generated_count

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 6: –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞–∑—É–º–Ω–∞—è
    avg_length = card_data.get("selected_examples_avg_length", 0)
    if avg_length > 50:
        issues.append(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è: {avg_length:.1f} —Å–ª–æ–≤")

    checks["issues"] = issues
    checks["has_issues"] = len(issues) > 0

    return checks


def display_card_details(card_data: dict, check_result: dict, sentences_lookup: dict):
    """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ä—Ç–æ—á–∫–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    lemma = card_data.get("lemma", "")
    pos = card_data.get("pos", "")
    card_full = card_data.get("card_full", {})

    print(f"\n{'='*80}")
    print(f"–ö–∞—Ä—Ç–æ—á–∫–∞ #{check_result['card_index'] + 1}: {lemma} ({pos})")
    print(f"{'='*80}")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–æ–≤–µ
    if card_full.get("synset_group"):
        print("\nüìñ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ª–æ–≤–µ:")
        print(f"  - Synset Group: {', '.join(card_full.get('synset_group', []))}")
        print(f"  - Primary Synset: {card_full.get('primary_synset', '')}")
        print(f"  - WordNet Definition: {card_full.get('wn_definition', '')[:100]}...")

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–≤–æ–¥
    print("\nüìù –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–≤–æ–¥:")
    print(f"  - –ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {card_full.get('simple_definition', 'N/A')}")
    print(f"    (–î–ª–∏–Ω–∞: {count_words(card_full.get('simple_definition', ''))} —Å–ª–æ–≤)")
    print(f"  - –ü–µ—Ä–µ–≤–æ–¥: {card_full.get('translation_ru', 'N/A')}")

    # –ü—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–Ω–∏–≥–∏
    selected_examples = card_full.get("selected_examples", [])
    if selected_examples:
        print(f"\nüìö –ü—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–Ω–∏–≥–∏ ({len(selected_examples)}):")
        for i, ex in enumerate(selected_examples, 1):
            word_count = count_words(ex)
            status = "‚úÖ" if word_count <= 50 else "‚ùå"
            print(f"  {status} –ü—Ä–∏–º–µ—Ä {i} ({word_count} —Å–ª–æ–≤):")
            print(f'      "{ex}"')
    elif card_full.get("valid_examples"):
        # –ï—Å–ª–∏ selected_examples –Ω–µ—Ç, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º valid_examples (–ø–µ—Ä–≤—ã–µ –∏–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö)
        valid_examples = card_full.get("valid_examples", [])
        print(f"\nüìö –í–∞–ª–∏–¥–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–Ω–∏–≥–∏ (–ø–µ—Ä–≤—ã–µ {len(valid_examples)}):")
        for i, ex in enumerate(valid_examples[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
            word_count = count_words(ex)
            status = "‚úÖ" if word_count <= 50 else "‚ùå"
            print(f"  {status} –ü—Ä–∏–º–µ—Ä {i} ({word_count} —Å–ª–æ–≤):")
            print(f'      "{ex}"')
        if len(valid_examples) > 3:
            print(f"  ... –∏ –µ—â–µ {len(valid_examples) - 3} –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    generated_examples = card_full.get("generated_examples", [])
    if generated_examples:
        print(f"\n‚ú® –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã ({len(generated_examples)}):")
        for i, ex in enumerate(generated_examples, 1):
            word_count = count_words(ex)
            status = "‚úÖ" if word_count <= 50 else "‚ùå"
            print(f"  {status} –ü—Ä–∏–º–µ—Ä {i} ({word_count} —Å–ª–æ–≤):")
            print(f'      "{ex}"')
    else:
        print(
            "\n‚ú® –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö (–Ω—É–∂–Ω–æ —Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º card_full)"
        )

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  - –ü—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –∫–Ω–∏–≥–∏: {card_data.get('selected_examples_count', 0)}")
    print(f"  - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {card_data.get('generated_examples_count', 0)}")
    print(f"  - –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {card_data.get('total_examples_count', 0)}")
    print(
        f"  - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∏–∑ –∫–Ω–∏–≥–∏: {card_data.get('selected_examples_avg_length', 0):.1f} —Å–ª–æ–≤"
    )
    print(f"  - –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –∏–∑ –∫–Ω–∏–≥–∏: {card_data.get('selected_examples_max_length', 0)} —Å–ª–æ–≤")
    print(
        f"  - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {card_data.get('generated_examples_avg_length', 0):.1f} —Å–ª–æ–≤"
    )
    print(f"  - –î–ª–∏–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {card_data.get('definition_length', 0)} —Å–ª–æ–≤")
    print(f"  - –ï—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥: {card_data.get('has_translation', False)}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∏:")
    checks = [
        ("–†–æ–≤–Ω–æ 3 –ø—Ä–∏–º–µ—Ä–∞", check_result.get("has_3_examples", False)),
        (
            "–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã (<=50 —Å–ª–æ–≤)",
            check_result.get("all_examples_appropriate_length", False),
        ),
        ("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–µ (<=15 —Å–ª–æ–≤)", check_result.get("definition_short", False)),
        ("–ï—Å—Ç—å –ø–µ—Ä–µ–≤–æ–¥", check_result.get("has_translation", False)),
    ]

    for check_name, result in checks:
        status = "‚úÖ" if result else "‚ùå"
        print(f"  {status} {check_name}")

    # –ü—Ä–æ–±–ª–µ–º—ã
    if check_result.get("issues"):
        print("\n‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º—ã:")
        for issue in check_result["issues"]:
            print(f"  - {issue}")
    else:
        print("\n‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

    print(f"\n{'='*80}")


def manual_review_batch(cards_batch: list[dict], batch_num: int, total_batches: int):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–∞—Ç—á–∞ –∫–∞—Ä—Ç–æ—á–µ–∫.

    Args:
        cards_batch: –°–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        batch_num: –ù–æ–º–µ—Ä –±–∞—Ç—á–∞ (–Ω–∞—á–∏–Ω–∞—è —Å 1)
        total_batches: –í—Å–µ–≥–æ –±–∞—Ç—á–µ–π
    """
    print(f"\n{'='*80}")
    print(f"–ë–ê–¢–ß {batch_num}/{total_batches} - {len(cards_batch)} –∫–∞—Ä—Ç–æ—á–µ–∫")
    print(f"{'='*80}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º sentences_lookup (—Ö–æ—Ç—è –æ–Ω –Ω–µ –Ω—É–∂–µ–Ω, –µ—Å–ª–∏ card_full —É–∂–µ –µ—Å—Ç—å)
    tokens_df = pd.read_parquet(TOKENS_PATH)
    sentences = reconstruct_sentences_from_tokens(tokens_df)
    sentences_df = create_sentences_dataframe(sentences)
    sentences_lookup = dict(zip(sentences_df["sentence_id"], sentences_df["sentence"]))

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É
    batch_results = []
    for idx, card_data in enumerate(cards_batch):
        card_index = (batch_num - 1) * BATCH_SIZE + idx
        check_result = check_card_quality(card_data, card_index)
        batch_results.append(check_result)

        display_card_details(card_data, check_result, sentences_lookup)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        print("\nüîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:")
        print("  1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ª–∏ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–Ω–∏–≥–∏ synset_group?")
        print("  2. –ù–µ—Ç –ª–∏ —Å–ø–æ–π–ª–µ—Ä–æ–≤ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö?")
        print("  3. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ª–∏ –ø—Ä–∏–º–µ—Ä—ã (—è—Å–Ω—ã–µ, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)?")
        print("  4. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ª–∏ –ø–µ—Ä–µ–≤–æ–¥?")
        print("  5. –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ?")

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏
        if idx < len(cards_batch) - 1:
            input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∫–∞—Ä—Ç–æ—á–∫–∏...")

    # –ò—Ç–æ–≥–∏ –±–∞—Ç—á–∞
    print(f"\n{'='*80}")
    print(f"–ò–¢–û–ì–ò –ë–ê–¢–ß–ê {batch_num}/{total_batches}")
    print(f"{'='*80}")

    total_in_batch = len(cards_batch)
    cards_with_issues = sum(1 for r in batch_results if r.get("has_issues", False))
    cards_ok = total_in_batch - cards_with_issues

    print(f"\n–í—Å–µ–≥–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –≤ –±–∞—Ç—á–µ: {total_in_batch}")
    print(f"‚úÖ –ë–µ–∑ –ø—Ä–æ–±–ª–µ–º: {cards_ok}")
    print(f"‚ö†Ô∏è  –° –ø—Ä–æ–±–ª–µ–º–∞–º–∏: {cards_with_issues}")

    if cards_with_issues > 0:
        print("\n–ö–∞—Ä—Ç–æ—á–∫–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏:")
        for result in batch_results:
            if result.get("has_issues", False):
                print(f"  - {result['lemma']} ({result['pos']}): {', '.join(result['issues'])}")

    return batch_results


def load_card_full_data(
    card_data: dict, cards_df: pd.DataFrame, sentences_lookup: dict, provider, cache
):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.

    –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∏–∑ aggregated_cards –∏ sentences_lookup.
    –ï—Å–ª–∏ card_full —É–∂–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ.
    """
    lemma = card_data.get("lemma", "")
    pos = card_data.get("pos", "")

    # –ï—Å–ª–∏ card_full —É–∂–µ –µ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if "card_full" in card_data and card_data["card_full"]:
        return card_data["card_full"]

    # –ò–Ω–∞—á–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ aggregated_cards
    row = (
        cards_df[cards_df["lemma"] == lemma].iloc[0] if lemma in cards_df["lemma"].values else None
    )
    if row is None:
        return {}

    # –ü–æ–ª—É—á–∞–µ–º synset_group
    synset_group = row.get("synset_group", [])
    if isinstance(synset_group, str):
        try:
            synset_group = json.loads(synset_group)
        except:
            synset_group = [synset_group] if synset_group else []

    primary_synset = row.get("primary_synset", "")
    wn_definition = row.get("definition", "")

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    sentence_ids = row.get("sentence_ids", [])
    if isinstance(sentence_ids, str):
        sentence_ids = json.loads(sentence_ids)

    examples = [
        (sid, sentences_lookup.get(sid, "")) for sid in sentence_ids if sid in sentences_lookup
    ]

    # –í–∞–ª–∏–¥–∞—Ü–∏—è synset_group
    if examples:
        from eng_words.validation import validate_examples_for_synset_group

        validation_result = validate_examples_for_synset_group(
            lemma=lemma,
            synset_group=synset_group,
            primary_synset=primary_synset,
            examples=examples,
            provider=provider,
            cache=cache,
        )

        valid_examples = [
            sentences_lookup[sid]
            for sid in validation_result["valid_sentence_ids"]
            if sid in sentences_lookup
        ]
    else:
        valid_examples = []

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–∞—Ä—Ç–æ—á–∫—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    # –ù–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–æ, –ø–æ—ç—Ç–æ–º—É –ø–æ–∫–∞ –≤–µ—Ä–Ω–µ–º —Ç–æ, —á—Ç–æ –µ—Å—Ç—å
    return {
        "synset_group": synset_group,
        "primary_synset": primary_synset,
        "wn_definition": wn_definition,
        "valid_examples": valid_examples[:5],  # –ü–µ—Ä–≤—ã–µ 5 –¥–ª—è –ø–æ–∫–∞–∑–∞
    }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    print("=" * 80)
    print("–†–£–ß–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ö–ê–†–¢–û–ß–ï–ö: –≠–¢–ê–ü 2.5")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    cards_df = pd.read_parquet(AGGREGATED_CARDS_PATH)
    tokens_df = pd.read_parquet(TOKENS_PATH)
    sentences = reconstruct_sentences_from_tokens(tokens_df)
    sentences_df = create_sentences_dataframe(sentences)
    sentences_lookup = dict(zip(sentences_df["sentence_id"], sentences_df["sentence"]))

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏)
    if FULL_CARDS_PATH.exists():
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ {FULL_CARDS_PATH}")
        results_df = pd.read_json(FULL_CARDS_PATH)
    elif TEST_RESULTS_PATH.exists():
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ {TEST_RESULTS_PATH}")
        logger.warning("–í —Ñ–∞–π–ª–µ –Ω–µ—Ç –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫. –î–ª—è –ø–æ–ª–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        logger.warning("  uv run python scripts/regenerate_cards_for_check.py")
        results_df = pd.read_json(TEST_RESULTS_PATH)
    else:
        logger.error(f"–§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {TEST_RESULTS_PATH} –∏–ª–∏ {FULL_CARDS_PATH}")
        sys.exit(1)

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(results_df)} —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫")

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    cards_list = results_df.to_dict("records")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    provider = get_provider("gemini", "gemini-3-flash-preview")
    cache = ResponseCache(cache_dir=CACHE_DIR, enabled=True)

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫, –≥–¥–µ –∏—Ö –Ω–µ—Ç
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫...")
    for card_data in tqdm(cards_list, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"):
        if "card_full" not in card_data or not card_data.get("card_full"):
            card_full = load_card_full_data(card_data, cards_df, sentences_lookup, provider, cache)
            card_data["card_full"] = card_full

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
    total_batches = (len(cards_list) + BATCH_SIZE - 1) // BATCH_SIZE

    print(f"\n–í—Å–µ–≥–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {len(cards_list)}")
    print(f"–ë–∞—Ç—á–µ–π: {total_batches} (–ø–æ {BATCH_SIZE} –∫–∞—Ä—Ç–æ—á–µ–∫)")
    print("\n–ë—É–¥–µ—Ç–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∫–∞—Ä—Ç–æ—á–∫–∏ –±–∞—Ç—á–∞–º–∏. –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.")

    input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏...")

    all_results = []

    for batch_num in range(1, total_batches + 1):
        start_idx = (batch_num - 1) * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(cards_list))
        batch = cards_list[start_idx:end_idx]

        batch_results = manual_review_batch(batch, batch_num, total_batches)
        all_results.extend(batch_results)

        if batch_num < total_batches:
            print(f"\n–ë–∞—Ç—á {batch_num} –∑–∞–≤–µ—Ä—à–µ–Ω. –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –±–∞—Ç—á—É...")
            input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞...")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n{'='*80}")
    print("–§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–í–ï–†–ö–ò")
    print(f"{'='*80}")

    total_cards = len(all_results)
    cards_ok = sum(1 for r in all_results if not r.get("has_issues", False))
    cards_with_issues = total_cards - cards_ok

    print(f"\n–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {total_cards}")
    print(f"‚úÖ –ë–µ–∑ –ø—Ä–æ–±–ª–µ–º: {cards_ok} ({cards_ok/total_cards*100:.1f}%)")
    print(f"‚ö†Ô∏è  –° –ø—Ä–æ–±–ª–µ–º–∞–º–∏: {cards_with_issues} ({cards_with_issues/total_cards*100:.1f}%)")

    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–±–ª–µ–º–∞–º
    if cards_with_issues > 0:
        issue_counts = {}
        for result in all_results:
            for issue in result.get("issues", []):
                issue_type = issue.split(":")[0]  # –ë–µ—Ä–µ–º —Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã
                issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1

        print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º:")
        for issue_type, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {issue_type}: {count}")

        print("\n–ö–∞—Ä—Ç–æ—á–∫–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏:")
        for result in all_results:
            if result.get("has_issues", False):
                print(f"  - {result['lemma']} ({result['pos']}) - {', '.join(result['issues'])}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
    check_results_file = OUTPUT_DIR / "manual_check_results.json"
    with open(check_results_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)

    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {check_results_file}")
    print(f"\n{'='*80}")
    print("–ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
