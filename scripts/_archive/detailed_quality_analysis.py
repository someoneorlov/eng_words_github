#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫.

–ü—Ä–æ–≤–æ–¥–∏—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏:
- –ù–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤
- –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- –ö–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
- –ö–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞
- –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤

Usage:
    uv run python scripts/detailed_quality_analysis.py --n 200
"""

import argparse
import json
import logging
import random
import re
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from nltk.corpus import wordnet as wn

from eng_words.llm.smart_card_generator import SmartCard
from eng_words.validation.example_validator import validate_card_examples

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class CardAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏."""
    card_index: int
    lemma: str
    pos: str
    synset_id: str
    
    # –ù–∞–ª–∏—á–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    has_examples: bool
    has_definition: bool
    has_translation: bool
    has_all_components: bool
    
    # –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤
    examples_valid: bool
    examples_count: int
    invalid_examples: list[str]
    found_forms: list[str]
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (1-5)
    examples_quality_score: int  # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    definition_quality_score: int  # –ü–æ–ª–Ω–æ—Ç–∞, —è—Å–Ω–æ—Å—Ç—å
    translation_quality_score: int  # –¢–æ—á–Ω–æ—Å—Ç—å, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å
    overall_quality_score: int  # –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    issues: list[str]
    critical_issues: list[str]  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
    
    # –î–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
    examples: list[str]
    definition: str
    translation: str
    
    # –ê–Ω–∞–ª–∏–∑
    definition_length: int
    translation_length: int
    examples_diversity: float  # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–æ–≤ (0-1)
    
    # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    examples_match_definition: bool
    translation_matches_definition: bool
    all_components_aligned: bool


def load_cards(path: Path) -> list[dict]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–∑ JSON."""
    logger.info(f"Loading cards from {path}")
    with open(path, "r", encoding="utf-8") as f:
        cards = json.load(f)
    logger.info(f"  Loaded {len(cards)} cards")
    return cards


def sample_cards(cards: list[dict], n: int, seed: int = 42) -> list[dict]:
    """–í—ã–±—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∫–∞—Ä—Ç–æ—á–µ–∫."""
    random.seed(seed)
    if len(cards) < n:
        logger.warning(f"Only {len(cards)} cards available, using all")
        return cards
    sampled = random.sample(cards, n)
    logger.info(f"Sampled {len(sampled)} cards for analysis")
    return sampled


def check_component_presence(card: dict) -> dict[str, bool]:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""
    has_examples = bool(card.get("selected_examples"))
    has_definition = bool(card.get("simple_definition") or card.get("wn_definition"))
    has_translation = bool(card.get("translation_ru"))
    has_all = has_examples and has_definition and has_translation
    
    return {
        "has_examples": has_examples,
        "has_definition": has_definition,
        "has_translation": has_translation,
        "has_all_components": has_all,
    }


def validate_examples(card: dict) -> dict[str, Any]:
    """–í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã."""
    try:
        smart_card = SmartCard(
            lemma=card.get("lemma", ""),
            pos=card.get("pos", ""),
            supersense=card.get("supersense", ""),
            selected_examples=card.get("selected_examples", []),
            excluded_examples=card.get("excluded_examples", []),
            simple_definition=card.get("simple_definition", ""),
            translation_ru=card.get("translation_ru", ""),
            generated_example=card.get("generated_example", ""),
            wn_definition=card.get("wn_definition", ""),
            book_name=card.get("book_name", "american_tragedy"),
            primary_synset=card.get("primary_synset", ""),
            synset_group=card.get("synset_group", [card.get("primary_synset", "")]),
        )
        validation = validate_card_examples(smart_card)
        
        return {
            "examples_valid": validation.is_valid,
            "examples_count": len(card.get("selected_examples", [])),
            "invalid_examples": validation.invalid_examples,
            "found_forms": validation.found_forms,
        }
    except Exception as e:
        logger.warning(f"Failed to validate examples for {card.get('lemma')}: {e}")
        return {
            "examples_valid": False,
            "examples_count": len(card.get("selected_examples", [])),
            "invalid_examples": card.get("selected_examples", []),
            "found_forms": [],
        }


def analyze_examples_quality(examples: list[str], lemma: str) -> int:
    """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (1-5)."""
    if not examples:
        return 1
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏:
    # 5: 5+ –ø—Ä–∏–º–µ—Ä–æ–≤, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ, lemma –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤–æ –≤—Å–µ—Ö
    # 4: 3-4 –ø—Ä–∏–º–µ—Ä–∞, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ, lemma –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    # 3: 2-3 –ø—Ä–∏–º–µ—Ä–∞, lemma –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    # 2: 1-2 –ø—Ä–∏–º–µ—Ä–∞ –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã
    # 1: –ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ
    
    lemma_lower = lemma.lower()
    lemma_in_all = all(lemma_lower in ex.lower() for ex in examples)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (–ø—Ä–∏–º–µ—Ä—ã –Ω–µ —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏)
    unique_starts = len(set(ex[:30].lower() for ex in examples))
    diversity = unique_starts / len(examples) if examples else 0
    
    if len(examples) >= 5 and lemma_in_all and diversity > 0.8:
        return 5
    elif len(examples) >= 3 and lemma_in_all and diversity > 0.7:
        return 4
    elif len(examples) >= 2 and lemma_in_all:
        return 3
    elif len(examples) >= 1 and lemma_in_all:
        return 2
    else:
        return 1


def analyze_definition_quality(definition: str, wn_definition: str = "") -> int:
    """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (1-5)."""
    if not definition:
        return 1
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏:
    # 5: –ü–æ–ª–Ω–æ–µ, —è—Å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (20+ —Å–∏–º–≤–æ–ª–æ–≤), –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç WordNet
    # 4: –•–æ—Ä–æ—à–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (15-20 —Å–∏–º–≤–æ–ª–æ–≤)
    # 3: –ü—Ä–∏–µ–º–ª–µ–º–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (10-15 —Å–∏–º–≤–æ–ª–æ–≤)
    # 2: –ö–æ—Ä–æ—Ç–∫–æ–µ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–æ–µ (5-10 —Å–∏–º–≤–æ–ª–æ–≤)
    # 1: –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    
    length = len(definition.strip())
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ—Ç—É (–Ω–µ –ø—Ä–æ—Å—Ç–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ)
    words = definition.split()
    is_simple = len(words) <= 3 or length < 10
    
    if length >= 20 and not is_simple:
        return 5
    elif length >= 15 and not is_simple:
        return 4
    elif length >= 10:
        return 3
    elif length >= 5:
        return 2
    else:
        return 1


def analyze_translation_quality(translation: str, definition: str) -> int:
    """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞ (1-5)."""
    if not translation:
        return 1
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏:
    # 5: –ü–æ–ª–Ω—ã–π, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ (10+ —Å–∏–º–≤–æ–ª–æ–≤)
    # 4: –•–æ—Ä–æ—à–∏–π –ø–µ—Ä–µ–≤–æ–¥ (8-10 —Å–∏–º–≤–æ–ª–æ–≤)
    # 3: –ü—Ä–∏–µ–º–ª–µ–º—ã–π –ø–µ—Ä–µ–≤–æ–¥ (5-8 —Å–∏–º–≤–æ–ª–æ–≤)
    # 2: –ö–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ (3-5 —Å–∏–º–≤–æ–ª–æ–≤)
    # 1: –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    
    length = len(translation.strip())
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤
    has_russian = bool(re.search(r'[–∞-—è—ë]', translation, re.IGNORECASE))
    
    if not has_russian:
        return 1
    
    if length >= 10:
        return 5
    elif length >= 8:
        return 4
    elif length >= 5:
        return 3
    elif length >= 3:
        return 2
    else:
        return 1


def check_alignment(card: dict) -> dict[str, bool]:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""
    examples = card.get("selected_examples", [])
    definition = card.get("simple_definition", "") or card.get("wn_definition", "")
    translation = card.get("translation_ru", "")
    lemma = card.get("lemma", "").lower()
    
    # –ü—Ä–∏–º–µ—Ä—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞: lemma –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö)
    examples_match = bool(examples) and all(lemma in ex.lower() for ex in examples)
    
    # –ü–µ—Ä–µ–≤–æ–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º)
    translation_matches = bool(translation) and bool(re.search(r'[–∞-—è—ë]', translation, re.IGNORECASE))
    
    # –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã
    all_aligned = examples_match and translation_matches and bool(definition)
    
    return {
        "examples_match_definition": examples_match,
        "translation_matches_definition": translation_matches,
        "all_components_aligned": all_aligned,
    }


def calculate_examples_diversity(examples: list[str]) -> float:
    """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ (0-1)."""
    if not examples or len(examples) == 1:
        return 0.0
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –ø—Ä–∏–º–µ—Ä–æ–≤
    starts = [ex[:50].lower().strip() for ex in examples]
    unique_starts = len(set(starts))
    
    return unique_starts / len(examples)


def identify_issues(card: dict, analysis: dict[str, Any]) -> tuple[list[str], list[str]]:
    """–í—ã—è–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—Ä—Ç–æ—á–∫–∏."""
    issues = []
    critical = []
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
    if not analysis["has_examples"]:
        critical.append("–ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤")
    if not analysis["has_definition"]:
        critical.append("–ù–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
    if not analysis["has_translation"]:
        critical.append("–ù–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞")
    if not analysis["examples_valid"]:
        critical.append(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: {analysis['invalid_examples']}")
    
    # –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞
    if analysis["examples_count"] == 0:
        issues.append("–ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤")
    elif analysis["examples_count"] == 1:
        issues.append("–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä")
    elif analysis["examples_count"] < 3:
        issues.append(f"–ú–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ ({analysis['examples_count']})")
    
    if analysis["examples_quality_score"] <= 2:
        issues.append("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    if analysis["definition_quality_score"] <= 2:
        issues.append("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
    
    if analysis["translation_quality_score"] <= 2:
        issues.append("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞")
    
    if not analysis["examples_match_definition"]:
        issues.append("–ü—Ä–∏–º–µ—Ä—ã –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é")
    
    if not analysis["translation_matches_definition"]:
        issues.append("–ü–µ—Ä–µ–≤–æ–¥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é")
    
    if analysis["examples_diversity"] < 0.5:
        issues.append("–ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    if analysis["definition_length"] < 10:
        issues.append("–û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
    
    if analysis["translation_length"] < 5:
        issues.append("–û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥")
    
    return issues, critical


def analyze_card(card: dict, index: int) -> CardAnalysis:
    """–î–µ—Ç–∞–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–¥–Ω—É –∫–∞—Ä—Ç–æ—á–∫—É."""
    logger.debug(f"Analyzing card {index + 1}: {card.get('lemma')}")
    
    # –ù–∞–ª–∏—á–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    components = check_component_presence(card)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤
    validation = validate_examples(card)
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    examples = card.get("selected_examples", [])
    definition = card.get("simple_definition", "") or card.get("wn_definition", "")
    translation = card.get("translation_ru", "")
    lemma = card.get("lemma", "")
    
    examples_quality = analyze_examples_quality(examples, lemma)
    definition_quality = analyze_definition_quality(definition, card.get("wn_definition", ""))
    translation_quality = analyze_translation_quality(translation, definition)
    
    # –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (—Å—Ä–µ–¥–Ω–µ–µ —Å —É—á–µ—Ç–æ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º)
    base_overall = (examples_quality + definition_quality + translation_quality) / 3
    if not components["has_all_components"] or not validation["examples_valid"]:
        overall_quality = max(1, base_overall - 2)  # –®—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
    else:
        overall_quality = base_overall
    
    # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    alignment = check_alignment(card)
    
    # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤
    diversity = calculate_examples_diversity(examples)
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    analysis_data = {
        "has_examples": components["has_examples"],
        "has_definition": components["has_definition"],
        "has_translation": components["has_translation"],
        "examples_valid": validation["examples_valid"],
        "examples_count": validation["examples_count"],
        "invalid_examples": validation["invalid_examples"],
        "examples_quality_score": examples_quality,
        "definition_quality_score": definition_quality,
        "translation_quality_score": translation_quality,
        "examples_match_definition": alignment["examples_match_definition"],
        "translation_matches_definition": alignment["translation_matches_definition"],
        "examples_diversity": diversity,
        "definition_length": len(definition),
        "translation_length": len(translation),
    }
    
    issues, critical = identify_issues(card, analysis_data)
    
    return CardAnalysis(
        card_index=index + 1,
        lemma=lemma,
        pos=card.get("pos", ""),
        synset_id=card.get("primary_synset", ""),
        has_examples=components["has_examples"],
        has_definition=components["has_definition"],
        has_translation=components["has_translation"],
        has_all_components=components["has_all_components"],
        examples_valid=validation["examples_valid"],
        examples_count=validation["examples_count"],
        invalid_examples=validation["invalid_examples"],
        found_forms=validation["found_forms"],
        examples_quality_score=examples_quality,
        definition_quality_score=definition_quality,
        translation_quality_score=translation_quality,
        overall_quality_score=round(overall_quality),
        issues=issues,
        critical_issues=critical,
        examples=examples,
        definition=definition,
        translation=translation,
        definition_length=len(definition),
        translation_length=len(translation),
        examples_diversity=diversity,
        examples_match_definition=alignment["examples_match_definition"],
        translation_matches_definition=alignment["translation_matches_definition"],
        all_components_aligned=alignment["all_components_aligned"],
    )


def generate_report(analyses: list[CardAnalysis]) -> dict[str, Any]:
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ."""
    total = len(analyses)
    
    # –°—Ä–µ–¥–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏
    avg_examples = sum(a.examples_quality_score for a in analyses) / total if total > 0 else 0
    avg_definition = sum(a.definition_quality_score for a in analyses) / total if total > 0 else 0
    avg_translation = sum(a.translation_quality_score for a in analyses) / total if total > 0 else 0
    avg_overall = sum(a.overall_quality_score for a in analyses) / total if total > 0 else 0
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
    def score_dist(score_func):
        return {i: sum(1 for a in analyses if score_func(a) == i) for i in range(1, 6)}
    
    # –ù–∞–ª–∏—á–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    has_all = sum(1 for a in analyses if a.has_all_components)
    has_examples = sum(1 for a in analyses if a.has_examples)
    has_definition = sum(1 for a in analyses if a.has_definition)
    has_translation = sum(1 for a in analyses if a.has_translation)
    
    # –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å
    valid_examples = sum(1 for a in analyses if a.examples_valid)
    aligned = sum(1 for a in analyses if a.all_components_aligned)
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    problematic = sum(1 for a in analyses if a.overall_quality_score <= 2)
    critical = sum(1 for a in analyses if a.critical_issues)
    
    # –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
    all_issues = []
    for a in analyses:
        all_issues.extend(a.issues)
    
    issue_freq = {}
    for issue in all_issues:
        issue_freq[issue] = issue_freq.get(issue, 0) + 1
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º
    avg_examples_count = sum(a.examples_count for a in analyses) / total if total > 0 else 0
    cards_with_many_examples = sum(1 for a in analyses if a.examples_count >= 5)
    cards_with_few_examples = sum(1 for a in analyses if a.examples_count < 3)
    
    return {
        "summary": {
            "total_analyzed": total,
            "average_scores": {
                "examples": round(avg_examples, 2),
                "definition": round(avg_definition, 2),
                "translation": round(avg_translation, 2),
                "overall": round(avg_overall, 2),
            },
            "score_distribution": {
                "examples": score_dist(lambda a: a.examples_quality_score),
                "definition": score_dist(lambda a: a.definition_quality_score),
                "translation": score_dist(lambda a: a.translation_quality_score),
                "overall": score_dist(lambda a: a.overall_quality_score),
            },
            "components_presence": {
                "all_components": has_all,
                "all_components_pct": has_all / total * 100 if total > 0 else 0,
                "has_examples": has_examples,
                "has_examples_pct": has_examples / total * 100 if total > 0 else 0,
                "has_definition": has_definition,
                "has_definition_pct": has_definition / total * 100 if total > 0 else 0,
                "has_translation": has_translation,
                "has_translation_pct": has_translation / total * 100 if total > 0 else 0,
            },
            "quality_metrics": {
                "valid_examples": valid_examples,
                "valid_examples_pct": valid_examples / total * 100 if total > 0 else 0,
                "aligned_components": aligned,
                "aligned_components_pct": aligned / total * 100 if total > 0 else 0,
                "problematic_cards": problematic,
                "problematic_cards_pct": problematic / total * 100 if total > 0 else 0,
                "critical_issues": critical,
                "critical_issues_pct": critical / total * 100 if total > 0 else 0,
            },
            "examples_statistics": {
                "average_count": round(avg_examples_count, 2),
                "cards_with_many": cards_with_many_examples,
                "cards_with_many_pct": cards_with_many_examples / total * 100 if total > 0 else 0,
                "cards_with_few": cards_with_few_examples,
                "cards_with_few_pct": cards_with_few_examples / total * 100 if total > 0 else 0,
            },
            "issue_frequency": dict(sorted(issue_freq.items(), key=lambda x: x[1], reverse=True)),
        },
        "analyses": [asdict(a) for a in analyses],
    }


def print_detailed_card(card: dict, analysis: CardAnalysis, index: int, total: int):
    """–í—ã–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ä—Ç–æ—á–∫–µ."""
    print("\n" + "=" * 100)
    print(f"–ö–ê–†–¢–û–ß–ö–ê {index + 1} / {total}")
    print("=" * 100)
    print()
    
    print(f"üìù –õ–ï–ú–ú–ê: {analysis.lemma} ({analysis.pos})")
    print(f"üîñ SYNSET: {analysis.synset_id}")
    print()
    
    # –°—Ç–∞—Ç—É—Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print("üì¶ –ö–û–ú–ü–û–ù–ï–ù–¢–´:")
    print(f"  –ü—Ä–∏–º–µ—Ä—ã:     {'‚úÖ' if analysis.has_examples else '‚ùå'} ({analysis.examples_count})")
    print(f"  –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {'‚úÖ' if analysis.has_definition else '‚ùå'} ({analysis.definition_length} —Å–∏–º–≤–æ–ª–æ–≤)")
    print(f"  –ü–µ—Ä–µ–≤–æ–¥:     {'‚úÖ' if analysis.has_translation else '‚ùå'} ({analysis.translation_length} —Å–∏–º–≤–æ–ª–æ–≤)")
    print()
    
    # –û—Ü–µ–Ω–∫–∏
    print("‚≠ê –û–¶–ï–ù–ö–ò –ö–ê–ß–ï–°–¢–í–ê (1-5):")
    print(f"  –ü—Ä–∏–º–µ—Ä—ã:     {analysis.examples_quality_score}/5")
    print(f"  –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {analysis.definition_quality_score}/5")
    print(f"  –ü–µ—Ä–µ–≤–æ–¥:     {analysis.translation_quality_score}/5")
    print(f"  –û–ë–©–ï–ï:       {analysis.overall_quality_score}/5")
    print()
    
    # –ü—Ä–∏–º–µ—Ä—ã
    if analysis.examples:
        print(f"üìö –ü–†–ò–ú–ï–†–´ ({analysis.examples_count}):")
        for i, ex in enumerate(analysis.examples[:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  {i}. {ex}")
        if len(analysis.examples) > 5:
            print(f"  ... –∏ –µ—â–µ {len(analysis.examples) - 5}")
        print(f"  –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {analysis.examples_diversity:.2f}")
        if analysis.found_forms:
            print(f"  –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã: {', '.join(analysis.found_forms)}")
        if analysis.invalid_examples:
            print(f"  ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ: {analysis.invalid_examples}")
    else:
        print("üìö –ü–†–ò–ú–ï–†–´: ‚ùå –ù–ï–¢")
    print()
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    print(f"üìñ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï:")
    if analysis.definition:
        print(f"  {analysis.definition}")
    else:
        print("  ‚ùå –ù–ï–¢")
    print()
    
    # –ü–µ—Ä–µ–≤–æ–¥
    print(f"üåê –ü–ï–†–ï–í–û–î:")
    if analysis.translation:
        print(f"  {analysis.translation}")
    else:
        print("  ‚ùå –ù–ï–¢")
    print()
    
    # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    print("üîó –°–û–ì–õ–ê–°–û–í–ê–ù–ù–û–°–¢–¨:")
    print(f"  –ü—Ä–∏–º–µ—Ä—ã ‚Üî –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {'‚úÖ' if analysis.examples_match_definition else '‚ùå'}")
    print(f"  –ü–µ—Ä–µ–≤–æ–¥ ‚Üî –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {'‚úÖ' if analysis.translation_matches_definition else '‚ùå'}")
    print(f"  –í—Å–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã:        {'‚úÖ' if analysis.all_components_aligned else '‚ùå'}")
    print()
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    if analysis.critical_issues:
        print("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
        for issue in analysis.critical_issues:
            print(f"  ‚ùå {issue}")
        print()
    
    if analysis.issues:
        print("‚ö†Ô∏è  –ü–†–û–ë–õ–ï–ú–´:")
        for issue in analysis.issues:
            print(f"  ‚Ä¢ {issue}")
        print()
    
    if not analysis.critical_issues and not analysis.issues:
        print("‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        print()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    parser = argparse.ArgumentParser(description="–î–µ—Ç–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫")
    parser.add_argument(
        "--input",
        type=Path,
        default=Path("data/synset_cards/synset_smart_cards_partial.json"),
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏",
    )
    parser.add_argument(
        "--n",
        type=int,
        default=200,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Seed –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/comparison/detailed_quality_analysis.json"),
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–í—ã–≤–æ–¥–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–µ",
    )
    
    args = parser.parse_args()
    
    print("=" * 100)
    print("–î–ï–¢–ê–õ–¨–ù–ê–Ø –ê–í–¢–û–ú–ê–¢–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ö–ê–†–¢–û–ß–ï–ö")
    print("=" * 100)
    print()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ä—Ç–æ—á–µ–∫
    cards = load_cards(args.input)
    
    if len(cards) < args.n:
        logger.warning(f"Only {len(cards)} cards available, analyzing all")
        args.n = len(cards)
    
    # –í—ã–±–æ—Ä–∫–∞
    sampled = sample_cards(cards, args.n, args.seed)
    
    # –ê–Ω–∞–ª–∏–∑
    logger.info("Starting detailed analysis...")
    analyses = []
    
    for i, card in enumerate(sampled):
        analysis = analyze_card(card, i)
        analyses.append(analysis)
        
        if args.verbose:
            print_detailed_card(card, analysis, i, len(sampled))
        elif (i + 1) % 20 == 0:
            logger.info(f"Analyzed {i + 1}/{len(sampled)} cards...")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = generate_report(analyses)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    args.output.parent.mkdir(parents=True, exist_ok=True)
    with open(args.output, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
    summary = report["summary"]
    print("\n" + "=" * 100)
    print("–°–í–û–î–ö–ê –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 100)
    print()
    print(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {summary['total_analyzed']} –∫–∞—Ä—Ç–æ—á–µ–∫")
    print()
    
    print("‚≠ê –°–†–ï–î–ù–ò–ï –û–¶–ï–ù–ö–ò –ö–ê–ß–ï–°–¢–í–ê (1-5):")
    print(f"  –ü—Ä–∏–º–µ—Ä—ã:       {summary['average_scores']['examples']:.2f}/5")
    print(f"  –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:   {summary['average_scores']['definition']:.2f}/5")
    print(f"  –ü–µ—Ä–µ–≤–æ–¥:       {summary['average_scores']['translation']:.2f}/5")
    print(f"  –û–ë–©–ï–ï:         {summary['average_scores']['overall']:.2f}/5")
    print()
    
    print("üì¶ –ù–ê–õ–ò–ß–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–û–í:")
    print(f"  –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:  {summary['components_presence']['all_components']} ({summary['components_presence']['all_components_pct']:.1f}%)")
    print(f"  –° –ø—Ä–∏–º–µ—Ä–∞–º–∏:     {summary['components_presence']['has_examples']} ({summary['components_presence']['has_examples_pct']:.1f}%)")
    print(f"  –° –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º:  {summary['components_presence']['has_definition']} ({summary['components_presence']['has_definition_pct']:.1f}%)")
    print(f"  –° –ø–µ—Ä–µ–≤–æ–¥–æ–º:     {summary['components_presence']['has_translation']} ({summary['components_presence']['has_translation_pct']:.1f}%)")
    print()
    
    print("‚úÖ –ö–ê–ß–ï–°–¢–í–û:")
    print(f"  –í–∞–ª–∏–¥–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã:        {summary['quality_metrics']['valid_examples']} ({summary['quality_metrics']['valid_examples_pct']:.1f}%)")
    print(f"  –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ:           {summary['quality_metrics']['aligned_components']} ({summary['quality_metrics']['aligned_components_pct']:.1f}%)")
    print(f"  –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ (‚â§2):         {summary['quality_metrics']['problematic_cards']} ({summary['quality_metrics']['problematic_cards_pct']:.1f}%)")
    print(f"  –° –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏: {summary['quality_metrics']['critical_issues']} ({summary['quality_metrics']['critical_issues_pct']:.1f}%)")
    print()
    
    print("üìö –ü–†–ò–ú–ï–†–´:")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ:  {summary['examples_statistics']['average_count']:.1f}")
    print(f"  –ú–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (‚â•5): {summary['examples_statistics']['cards_with_many']} ({summary['examples_statistics']['cards_with_many_pct']:.1f}%)")
    print(f"  –ú–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (<3):  {summary['examples_statistics']['cards_with_few']} ({summary['examples_statistics']['cards_with_few_pct']:.1f}%)")
    print()
    
    if summary['issue_frequency']:
        print("‚ö†Ô∏è  –ß–ê–°–¢–´–ï –ü–†–û–ë–õ–ï–ú–´ (—Ç–æ–ø-10):")
        for issue, count in list(summary['issue_frequency'].items())[:10]:
            print(f"  ‚Ä¢ {issue}: {count} —Ä–∞–∑(–∞)")
        print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5 –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫
    problematic = sorted(analyses, key=lambda a: a.overall_quality_score)[:5]
    if problematic:
        print("üö® –¢–û–ü-5 –ü–†–û–ë–õ–ï–ú–ù–´–• –ö–ê–†–¢–û–ß–ï–ö:")
        for a in problematic:
            print(f"  {a.lemma} ({a.pos}): {a.overall_quality_score}/5 - {', '.join(a.critical_issues[:2])}")
        print()
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output}")
    print()
    print("=" * 100)
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("=" * 100)


if __name__ == "__main__":
    main()

