#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã aggregated_cards.parquet –¥–ª—è –≠—Ç–∞–ø–∞ 0.1 –ø–ª–∞–Ω–∞ QUALITY_FILTERING_PLAN.

–ò–∑—É—á–∞–µ—Ç:
- –°—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–ª–æ–Ω–æ–∫
- –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
- –°–≤—è–∑—å –º–µ–∂–¥—É sentence_ids –∏ synset_group
- –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
"""

import json
import sys
from pathlib import Path

import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

def analyze_aggregated_cards():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É aggregated_cards.parquet."""
    path = Path("data/synset_aggregation_full/aggregated_cards.parquet")
    
    if not path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        return
    
    print("=" * 70)
    print("–ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ AGGREGATED CARDS")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_parquet(path)
    
    print(f"\nüìä –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
    print(f"  Shape: {df.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
    
    print(f"\nüìã –ö–æ–ª–æ–Ω–∫–∏:")
    for i, col in enumerate(df.columns, 1):
        print(f"  {i}. {col}")
    
    print(f"\nüîç –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
    for col, dtype in df.dtypes.items():
        print(f"  {col}: {dtype}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
    print(f"\nüìù –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏:")
    first_row = df.iloc[0]
    for col in df.columns:
        value = first_row[col]
        if isinstance(value, (list, dict)):
            print(f"  {col}: {type(value).__name__} (len={len(value) if hasattr(value, '__len__') else 'N/A'})")
            if isinstance(value, list) and len(value) > 0:
                print(f"    –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç: {value[0]}")
            elif isinstance(value, dict):
                print(f"    –ö–ª—é—á–∏: {list(value.keys())[:5]}")
        else:
            print(f"  {col}: {value}")
    
    # –ê–Ω–∞–ª–∏–∑ synset_group
    if 'synset_group' in df.columns:
        print(f"\nüîó –ê–Ω–∞–ª–∏–∑ synset_group:")
        sample_groups = df['synset_group'].head(5)
        for idx, group in enumerate(sample_groups, 1):
            print(f"  –ü—Ä–∏–º–µ—Ä {idx}: {group} (type: {type(group).__name__})")
            if isinstance(group, list):
                print(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ synsets –≤ –≥—Ä—É–ø–ø–µ: {len(group)}")
                print(f"    Synsets: {group[:3]}...")
    
    # –ê–Ω–∞–ª–∏–∑ sentence_ids
    if 'sentence_ids' in df.columns:
        print(f"\nüìö –ê–Ω–∞–ª–∏–∑ sentence_ids:")
        sample_sentences = df['sentence_ids'].head(5)
        for idx, sids in enumerate(sample_sentences, 1):
            print(f"  –ü—Ä–∏–º–µ—Ä {idx}: {type(sids).__name__}")
            if isinstance(sids, list):
                print(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ sentence_ids: {len(sids)}")
                print(f"    –ü–µ—Ä–≤—ã–µ 5: {sids[:5]}")
            elif pd.notna(sids):
                print(f"    –ó–Ω–∞—á–µ–Ω–∏–µ: {sids}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ synset_group
    if 'synset_group' in df.columns:
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ synset_group:")
        # –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ synsets –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ
        group_sizes = []
        for group in df['synset_group']:
            if isinstance(group, list):
                group_sizes.append(len(group))
            elif isinstance(group, str):
                # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
                try:
                    parsed = json.loads(group)
                    if isinstance(parsed, list):
                        group_sizes.append(len(parsed))
                    else:
                        group_sizes.append(1)
                except:
                    group_sizes.append(1)
            else:
                group_sizes.append(1)
        
        if group_sizes:
            print(f"  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã: {sum(group_sizes) / len(group_sizes):.2f}")
            print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(group_sizes)}")
            print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(group_sizes)}")
            print(f"  –ì—Ä—É–ø–ø—ã —Å 1 synset: {sum(1 for s in group_sizes if s == 1)} ({sum(1 for s in group_sizes if s == 1)/len(group_sizes)*100:.1f}%)")
            print(f"  –ì—Ä—É–ø–ø—ã —Å 2+ synsets: {sum(1 for s in group_sizes if s > 1)} ({sum(1 for s in group_sizes if s > 1)/len(group_sizes)*100:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ sentence_ids
    if 'sentence_ids' in df.columns:
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ sentence_ids:")
        sentence_counts = []
        for sids in df['sentence_ids']:
            if isinstance(sids, list):
                sentence_counts.append(len(sids))
            elif pd.notna(sids):
                sentence_counts.append(1)
            else:
                sentence_counts.append(0)
        
        if sentence_counts:
            print(f"  –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {sum(sentence_counts) / len(sentence_counts):.2f}")
            print(f"  –ú–∏–Ω–∏–º—É–º: {min(sentence_counts)}")
            print(f"  –ú–∞–∫—Å–∏–º—É–º: {max(sentence_counts)}")
            print(f"  –ö–∞—Ä—Ç–æ—á–∫–∏ –±–µ–∑ –ø—Ä–∏–º–µ—Ä–æ–≤: {sum(1 for c in sentence_counts if c == 0)} ({sum(1 for c in sentence_counts if c == 0)/len(sentence_counts)*100:.1f}%)")
    
    print("\n" + "=" * 70)
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
    print("=" * 70)


if __name__ == "__main__":
    analyze_aggregated_cards()
